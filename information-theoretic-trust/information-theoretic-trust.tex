\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}

\lhead{Information-theoretic trust; \textsc{OpenAI Trust in AI Report}}
\rhead{Adithya C. Ganesh}
\pagestyle{fancy}

%
%Works cited environment
%(to start, use \begin{workscited...}, each entry preceded by \bibent)
% - from Ryan Alcock's MLA style file
%
\newcommand{\bibent}{\noindent \hangindent 40pt}
\newenvironment{workscited}{\newpage \begin{center} \textsc{References} \end{center}}{\newpage }

\title{Information-theoretic tools for trust \\ \vspace{0.3cm} OpenAI Trust in AI Report}
\author{\textsc{Adithya C. Ganesh}}

\begin{document}

Political scientists identify two categories of problems that impede trust: {\it information problems} and {\it commitment problems} \cite{frieden2010world}.  While the simplest application is perhaps the prisoner's dilemma, these categories play a critical role in understanding problems of bargaining in war \cite{lake2010two}, iterated games \cite{axelrod1981evolution}, and economic policy \cite{panagariya2003international}.

For instance, the current U.S.--China trade war presents a setting in which two parties converge on an ostensibly suboptimal policy (economic sanctions) because of information and commitment problems \cite{liu2018understanding, amiti2019impact, li2018economic}.  First, each party lacks complete information on the long-term objectives of the other (e.g.\ geopolitical / economic agendas).  Second, each party cannot be sure that the other will commit to policy changes (e.g.\ reduction in trade deficit or an improvement in Chinese market practices).  Modelling the behavior of agents with information theory might play a role in resolving this impasse or, more broadly, in building AI we can trust. 

There are three core facets of ``information-theoretic trust'': {\it compression}, {\it communication}, and {\it inference}.  While these are standard topics in introductory texts \cite{cover2012elements, mackay2003information}, it is worth expanding on the connections between information theory and trust.  I will elaborate on these three topics in the next few paragraphs.

First, compression plays a critical role in safe interaction with an ensemble of agents.  Codes have played an essential role in history, e.g.\ see the prominent role of the Enigma machine in World War II \cite{booss2003mathematics}.  Using codes in practice requires understanding how much information we can safely encode in a message.

Shannon's source coding theorem states that $N$ i.i.d.\ random variables with entropy $H(X)$ can be compressed into more than $N H(X)$ bits with negligible risk of information loss as $N \to \infty$ \cite{shannon1948mathematical, mackay2003information}.  Conversely, if they are compressed into fewer than $N H(X)$ bits, it is virtually certain that information will be lost.  The language of compression is a powerful framework to analyze dimensionality reduction models like principal component analysis \cite{geiger2012relative} and (variational) autoencoders \cite{doersch2016tutorial}.

Second, safely interacting with AI requires trust in our communication channels.  While the source coding theorem provides a useful framework to analyze the lossless case, in practice, we often deal with noisy channels.  ``Nuclear close calls'' present useful case studies to examine in history, wherein nuclear weapons were ``nearly'' deployed by a legitimate authority \cite{tertrais2017brink}.  In these settings, state actors are unsure of the intentions of others.  The Norwegian rocket launch of 1995, described as a ``poster child for nuclear dangers'' (Tertrais \cite{tertrais2017brink}), involved Norwegian and American scientists launching a rocket to study weather data.  While information about this launch was sent to Moscow, before it reached the authorities, the launch was interpreted as a sign of a possible adversarial strike.

Shannon's noisy-channel coding theorem formalizes the notion of channel capacity, a measure of how much we can safely transmit over a noisy channel.  We first review the {\it mutual information,} which provides a measure of how much information is communicated over a channel.  If $X, Y$ are random variables representing the input and output to a noisy channel, then the mutual information quantifies how much information the output conveys about the input: $I(X; Y) = H(X) - H(X | Y) = H(Y) - H(Y|X)$.  The {\it channel capacity} of a channel $Q$ can be computed as $C(Q) = \max_{P_X} I(X; Y)$, where the maximization is over all possible input ensembles $P_X$.  Intuitively, $C(Q)$ represents the number of bits that can be sent over a channel with arbitrarily low decoding error for the recipient.

Finally, trusting AI requires us to model and infer the behavior of complex agents.  We might initially ask which priors are most accurate in settings of incomplete information.  There are many potential answers to this question, but information theory suggests that the ``maximum-entropy distribution'' makes the fewest assumptions \cite{adi-maxent, mackay2003information}.  

One can derive the familiar probability distributions using this idea.  With fixed variance, the Gaussian is the maximum entropy prior; with fixed mean on $(0, \infty)$, the exponential distribution is the maximum entropy prior.  There are an entire class of methods, ``maximum entropy methods,'' with applications in time series forecasting, statistical mechanics, and inference more broadly \cite{jaynes1982rationale, jaynes2003probability}.

While model interpretability techniques \cite{hohman2018visual, koh2017understanding, olah2017feature} have shed light on how to understand agents, information-theoretic models are still powerful tools we can apply to model the multi-agent case.  Building a future with safe, trustworthy AI may require deep inquiry and understanding of the information-theoretic toolbox.

\newpage

\section*{Acknowledgments}

Thanks to Yuval Wigderson, Michael Swerdlow, and Jordan Alexander for reading drafts of this research note.

\bibliographystyle{plain}
\nocite{*}
\bibliography{refs}

\end{document}
